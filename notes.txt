New machine setup:

Make sure that the NVidia SDK manager finishes its post-flash SDK install, or the libraries needed to build will be missing.
If it doesn't already exist, unpack Tegra_Multimedia_API_R32.2.3_aarch64.tbz2 (or a newer version, from NV developer downloads) to /usr/src, creating /usr/src/tegra_multimedia_api


Don't forget to set up the submodules after cloning:
  git submodule update --init --recursive

Install prerequisite packages:
  sudo apt install scons libhidapi-dev libhidapi-hidraw0 libboost-all-dev libudev-dev clang lldb ninja-build libusb-1.0-0-dev libepoxy-dev

If you're on L4T, get a new enough cmake for the DepthAI build (3.10.2 as shipped in L4T/bionic is too old):
  sudo apt remove cmake*
  sudo apt install python3-pip
  sudo -H pip3 install --upgrade pip
  sudo -H pip3 install cmake

Additional prereqs for the RDMA framework and rdma-client
  sudo apt install libsdl2-dev librdmacm-dev

Optional: If you want to build the DepthAI backend, run the included script to build required libraries first:
  ./build-depthai-core.sh

udev permissions:

# udev rules for Vive (contents of /etc/udev/rules.d/83-hmd.rules )
# update and run `sudo udevadm control --reload`

SUBSYSTEM=="usb", ATTR{idVendor}=="0bb4", MODE="0666", GROUP="plugdev"
SUBSYSTEM=="usb", ATTR{idVendor}=="28de", MODE="0666", GROUP="plugdev"
SUBSYSTEM=="hidraw", ATTRS{idVendor}=="0bb4", MODE="0666", GROUP="plugdev"
SUBSYSTEM=="hidraw", ATTRS{idVendor}=="28de", MODE="0666", GROUP="plugdev"

# Microsoft HoloLens Sensors - USB
ATTRS{idVendor}=="045e", ATTRS{idProduct}=="0659", MODE="0666", GROUP="plugdev"
# Lenovo Explorer control interface - USB
ATTRS{idVendor}=="17ef", ATTRS{idProduct}=="b800", MODE="0666", GROUP="plugdev"
ATTRS{idVendor}=="17ef", ATTRS{idProduct}=="b801", MODE="0666", GROUP="plugdev"

# Intel Movidius Myraid VPU
SUBSYSTEM=="usb", ATTRS{idVendor}=="03e7", MODE="0666", GROUP="plugdev"


Make sure that the user you're running as is members of the "plugdev" and "input" groups.
plugdev is required to open the Vive's /dev/hidraw* node
input is required to open nodes under /dev/input for the USB-HID remote control

To avoid accidentally powering off the Jetson with the USB-HID remote, disable systemd's response to power/suspend/hibernate/whatever keys:
edit /etc/systemd/logind.conf :

[Login]
HandlePowerKey=ignore
HandleSuspendKey=ignore
HandleHibernateKey=ignore
HandleLidSwitch=ignore
HandleLidSwitchDocked=ignore

then sudo systemctl restart systemd-logind #(NOTE: this will kill your login session!)

Disabling the desktop environment:
sudo systemctl isolate multi-user.target            # takes effect immediately
sudo systemctl set-default multi-user.target        # to persist across reboots

Reenabling the desktop environment
sudo systemctl isolate graphical.target             # takes effect immediately
sudo systemctl set-default graphical.target         # to persist across reboots



Camera ISP settings are stored at /var/nvidia/nvcam/settings/camera_overrides.isp
The vendor ISP settings may be adjusted to improve flicker correction. These settings have been found to work well for capturing at 90fps and correcting for 60hz flicker:
flicker.ConfidenceThreshold = 32;
flicker.SuccessFrameCount = 8;
flicker.FailureFrameCount = 3;
flicker.CorrectionFreqListEntries = 4;
flicker.CorrectionFreqList = {60, 90, 100, 120};

The default Sharpening filter should be disabled to improve accuracy of Charuco target detection:
sharpness.v5.enable = FALSE;



====== Nvidia Visual Profiler (nvvp) (on desktop) =====
CUDA 11.1 + Ubuntu 20.04 nvvp will fail to launch out-of-the-box with a Java error.
solution: nvvp needs to run under the java 8 runtime (and as root, typically)

  sudo apt install openjdk8-jre
  sudo update-alternatives --config java
  select OpenJDK 8

nvvp can then be run with sudo /usr/local/cuda/bin/nvvp

Sample invocation to generate an nvprof tracefile:

(as root, on device)
  nvprof --system-profiling on --timeout 15 --cpu-profiling on --export-profile /tmp/dgpu-worker.nvvp /home/dweatherford/hmdcam/build/bin/dgpu-worker

Sample invocation for profiling hmdcam + dgpu-worker together. Requires CWD to be correct so hmdcam can load shaders.
(as root, on device)
  cd /home/dweatherford/hmdcam
  nvprof --profile-child-processes --system-profiling on --timeout 15 --cpu-profiling on --export-profile /tmp/hmdcam-%p.nvvp /home/dweatherford/hmdcam/build/bin/hmdcam


Copy /tmp/dgpu-worker.nvvp back to the host and load it into nvvp.


